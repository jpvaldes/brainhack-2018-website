<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="author" content="BrainHack Magdeburg" />
  <title>BrainHack Magdeburg &ndash; Project Ideas</title>

  <!-- once upon a time, favicons were a simple folk; but then they grew to be greedy... -->
  <link rel="apple-touch-icon" sizes="180x180" href="./apple-touch-icon.png?v=E6j37mrY2e">
  <link rel="icon" type="image/png" sizes="32x32" href="./favicon-32x32.png?v=E6j37mrY2e">
  <link rel="icon" type="image/png" sizes="16x16" href="./favicon-16x16.png?v=E6j37mrY2e">
  <link rel="manifest" href="./site.webmanifest?v=E6j37mrY2e">
  <link rel="mask-icon" href="./safari-pinned-tab.svg?v=E6j37mrY2e" color="#ff9e0d">
  <link rel="shortcut icon" href="./favicon.ico?v=E6j37mrY2e">
  <meta name="apple-mobile-web-app-title" content="BrainHack Magdeburg">
  <meta name="application-name" content="BrainHack Magdeburg">
  <meta name="msapplication-TileColor" content="#ff9e0d">
  <meta name="theme-color" content="#ff9e0d">

  <link rel="stylesheet" type="text/css" href="./theme/css/style.css">
</head>
<body>
  <main>
  <nav class='card'>
    <ul>
      <li><a href="./index.html" >home</a></li>
      <li><a href="./projects.html"  class='active' >projects</a></li>
      <li><a href="./details.html" >details</a></li>
    </ul>
  </nav>

  
  <header id="project-landing" class="card">
<svg viewbox="0 0 215.059 79.375">
  <path d="M173.907 46.773l-8.393 9.21v23.382h-15.37V.01h15.37v36.353l7.194-9.701L192.002.01h18.748l-26.705 35.1 27.686 44.255h-18.203z" />
  <path d="M173.907 46.773l10.138-11.663 27.686 44.255h-18.203z" />
  <path d="M155.516 54.512q-.381 5.995-2.616 10.791-2.18 4.796-5.886 8.175-3.706 3.38-8.83 5.233-5.123 1.798-11.336 1.798-7.03 0-12.48-2.562-5.45-2.616-9.157-7.248-3.706-4.687-5.668-11.227-1.962-6.54-1.962-14.389V34.401q0-7.903 2.07-14.443 2.072-6.54 5.887-11.227 3.815-4.687 9.211-7.249 5.45-2.616 12.208-2.616 6.54 0 11.61 1.853 5.122 1.853 8.72 5.287 3.596 3.433 5.667 8.393 2.072 4.96 2.617 11.173h-15.26q-.219-3.543-1.09-6.214-.818-2.67-2.453-4.414-1.581-1.799-4.034-2.67-2.398-.927-5.777-.927-3.542 0-6.158 1.526-2.616 1.526-4.36 4.469-1.69 2.943-2.508 7.249-.817 4.25-.817 9.7v10.792q0 11.718 3.27 17.332 3.325 5.613 10.464 5.613 5.995 0 9.484-3.16 3.488-3.216 3.87-10.356z" />
  <path d="M88.394 62.796H64.576L59.89 79.365H43.757L69.754.01h13.898l25.507 79.355H93.026zM68.337 49.77h16.46l-8.121-29.212z" />
  <path d="M52.812 79.365H37.66v-34.39H15.206v34.39H0V.01h15.206v32.538H37.66V.01h15.152z" />
</svg>    <div class="blurb">
      <h1>Project <span class="bold">Ideas</span></h1>
      <p>For now these are only ideas. You can help make them happen.</p>
    </div>
  </header>

    <!-- Available info icons:
      <li class='icon-project'><a href='#'>Project Page</a></li>
      <li class='icon-people'>Name, Another Name, So Popular</li>
      <li class='icon-contact'>email@address.com</li>
      <li class='icon-link'><a href='#'>a link</a></li>
    -->
    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_datalad.png" />
        <h2>Research Data Management with DataLad</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Michael Hanke</li>
        <li class="icon-contact">michael.hanke@gmail.com</li>
        <li class="icon-link"><a href="http://datalad.org">datalad.org</a></li>
      </ul>
      <p>Research data management is crucial for projects of any size. This
      <span class="tutorial">tutorial</span> introduces you to DataLad, an
      open-source solution for decentralized data management that is build atop
      Git. DataLad's features make it a perfect fit for tracking your research
      output, whether it's produced on your laptop or in "the cloud". DataLad
      helps you get ready for open-science from the very start.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_pymvpa.png" />
        <h2>BIDS-app-Based Searchlight Analysis Pipeline</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Michael Hanke</li>
        <li class="icon-contact">michael.hanke@gmail.com</li>
        <li class="icon-link"><a href="http://bids-apps.neuroimaging.io">BIDS-app</a></li>
        <li class="icon-link"><a href="http://www.pymvpa.org">PyMVPA</a></li>
      </ul>
      <p>Decoding analyses using a traveling "searchlight" are a widely used
      approach to map the availability of a given signal, and can also be used
      for a representational similarity analysis (RSA). This
      <span class="project">project</span> aims to produce a BIDS-app for a
      PyMVPA-based searchlight analysis for decoding and RSA.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_ph_brain.png" />
        <h2>Basics of Image Reconstruction: From Raw Data to the Image</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Falk Lüsebrink</li>
        <li class="icon-contact">falk.luesebrink@ovgu.de</li>
      </ul>
      <p>In this <span class="project">project</span>, a basic reconstruction
      pipeline is to be developed for Cartesian sampled gradient echo data. This
      includes reading raw data, k space filtering, data synthesis in case of
      partial Fourier, coil combination (sum of squares and adaptive
      combination) and writing of image files. The aim is to reconstruct image
      with at least the quality of the vendors.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_nipype.png" />
        <h2>Containerized Nipype Pipelines on the Cluster</h2>
      </header>
      <ul class="info">
        <li class="icon-people">José P. Valdés-Herrera</li>
        <li class="icon-contact">jose.valdes@dzne.de</li>
        <li class="icon-link"><a href="http://nipype.readthedocs.io/">Nipype</a></li>
        <li class="icon-link"><a href="https://www.docker.com">Docker</a></li>
        <li class="icon-link"><a href="http://singularity.lbl.gov">Singularity</a></li>
      </ul>
      <p>This <span class="project">project</span> proposes taking custom or
      already available pipelines developed using Nipype, package them in a
      container using Docker or Singularity, and integrate them with cluster
      software (SLURM, SGE, Condor). The aim is to automate the process of
      taking those pipelines to a cluster as much as possible, either by
      developing scripts or extending Nipype's functionality.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_dask.png" />
        <h2>Machine Learning with Dask and Neuroimaging Data</h2>
      </header>
      <ul class="info">
        <li class="icon-people">José P. Valdés-Herrera</li>
        <li class="icon-contact">jose.valdes@dzne.de</li>
        <li class="icon-link"><a href="https://dask.pydata.org/">Dask</a></li>
      </ul>
      <p>The aim of this <span class="project">project</span> is to analyse
      large neuroimaging datasets &mdash; that do not fit in memory &mdash; even
      without access to powerful machines or clusters. To achieve this, we can
      use machine learning libraries (Nilearn, PyMVPA, scikit-learn) together
      with a library like Dask. During the event, the goal is to integrate
      these tools, pick a large dataset, and analyse it on "modest" hardware
      (e.g. a laptop).</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_ph_limes.jpg" />
        <h2>Motor Imagery Classification with Deep Neural Networks</h2>
      </header>
      <ul class="info">
        <li class="icon-people">JiaHua Xu</li>
        <li class="icon-contact">jiahua.xu@med.ovgu.de</li>
        <li class="icon-link"><a href="http://www.bbci.de/competition/iv/#dataset">Datasets</a></li>
        <li class="icon-link"><a href="https://www.martinos.org/mne/stable/index.html">MNE</a></li>
        <li class="icon-link"><a href="http://scikit-learn.org/stable/">scikit-learn</a></li>
      </ul>
      <p>Motor imagery represents frequency phenomenon of Event-Related
      Desynchronization (ERD) and Event-Related Synchronization (ERS) in the
      motor cortex. In this <span class="tutorial">tutorial</span>, you will
      jointly train a logistic regression and a deep neural network combined
      model (Deep&amp;Wide Learning) to evaluate the performance on four classes
      of motor imagery data.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_ph_paint.jpg" />
        <h2>Brain Network Analysis Pipelines in Neuroimaging Data</h2>
      </header>
      <ul class="info">
        <li class="icon-people">JiaHua Xu</li>
        <li class="icon-contact">jiahua.xu@med.ovgu.de</li>
        <li class="icon-link"><a href="http://www.fieldtriptoolbox.org/">FieldTrip</a></li>
        <li class="icon-link"><a href="https://www.martinos.org/mne/stable/index.html">MNE</a></li>
        <li class="icon-link"><a href="https://sites.google.com/site/bctnet/">Brain Connectivity Toolbox</a></li>
      </ul>
      <p>This <span class="project">project</span> will focus on the pipelines
      of brain network analysis when using EEG data with different kinds of
      graphics measures, from mathematics to practical. The aim is to help
      visualize the brain dynamic connectivity changes and understand the
      network structures of brain activities in a different perspective.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_brain.jpg" />
        <h2>Highest Resolution T1-weighted In Vivo Human Brain MRI Data</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Falk Lüsebrink</li>
        <li class="icon-contact">falk.luesebrink@ovgu.de</li>
        <li class="icon-link"><a href="http://www.nature.com/articles/sdata201732">publication</a></li>
      </ul>
      <p>In this <span class="tutorial">tutorial</span>, the so far highest
      resolution T1-weighted in vivo human brain MRI data will be presented.
      Details will be given on what the entire data set consists of, how it was
      acquired, how it will be extended in the near future, and what potential
      use cases of it are. Besides simply showing some astonishing images of the
      brain.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_ph_dots.jpg" />
        <h2>How to Open Data (at OvGU)</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Falk Lüsebrink</li>
        <li class="icon-contact">falk.luesebrink@ovgu.de</li>
      </ul>
      <p>Sharing your data openly is fantastic and may be beneficial for
      everyone. However, to improve visibility and enable citeability, data has
      to be provided with a digital object identifier (DOI). This <span class="tutorial">tutorial</span> will guide you through the steps to
      successfully apply for a DOI at the OvGU university library. Starting from
      the application itself, how and where to host data, and the regulations.
      Non-OvGU-members will learn how your institute is able to provide
      DOIs.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_audio.png" />
        <h2>Hear What I Hear: Reconstructing the Music in Our Heads</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Andrew Curran</li>
        <li class="icon-contact">andrew.curran@med.ovgu.de</li>
        <li class="icon-link"><a href="http://www.eneuro.org/content/early/2018/01/29/ENEURO.0358-17.2018">Visual Image Reconstruction</a></li>
      </ul>
      <p>Recently, research groups have shown that it is possible to read images
      from a person's brain as they are seen using both fMRI and EEG through
      machine learning strategies. These strategies have however not yet been
      applied to auditory stimuli. It should theoretically also be possible to
      record imagined visual or auditory perceptions. This
      <span class="project">project</span> aims to utilize the same principles
      to extract simple musical details through EEG, and potentially even
      imagined music.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_nipype2.png" />
        <h2>Nipype Tutorial</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Michael Notter</li>
        <li class="icon-contact">michaelnotter@hotmail.com</li>
        <li class="icon-link"><a href="https://miykael.github.io/nipype_tutorial/">Nipype Tutorial</a></li>
      </ul>
      <p>This <span class="tutorial">tutorial</span> introduces you to Nipype,
      an open-source neuroimaging software written in Python. Nipype provides a
      unified way of interfacing with heterogeneous neuroimaging software like
      SPM, FSL, FreeSurfer, AFNI, ANTS, Camino, and many more. It allows users
      to create flexible and complex workflows very quickly, that then can be
      executed in parallel either locally or on any computational cluster.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_3d.png" />
        <h2>3D-Printing for Science</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Michael Lippert</li>
        <li class="icon-contact">mlippert@lin-magdeburg.de</li>
      </ul>
      <p>Additive manufacturing provides exciting opportunities in the lab:
         need a prototype? Just print it! More of these super expensive small
         parts that evil ACME Corp overcharges you for? 3D-printers for the
         rescue! In this <span class="project">project</span> we will take a
         look at the workflow of FDM 3D printing and how you can implement it in
         your lab.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_vr.png" />
        <h2>Seeing is Believing: Connecting VR Environments to EEG Output Interpretation</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Siew-Wan Ohl; JiaHua Xu</li>
        <li class="icon-contact">siew-wan.ohl@ovgu.de; jiahua.xu@med.ovgu.de</li>
        <li class="icon-link"><a href="http://www.edu2vr.com">Edu2VR</a></li>
        <li class="icon-link"><a href="http://openbci.com">OpenBCI</a></li>
      </ul>
      <p>For this <span class="project">project</span>, we would like to see
      what EEG signals will be generated when different VR environments or
      scenarios are presented to the user. It is up to the creativity of the
      programmer to either generate interesting VR environments or directly
      import existing 360 degree photos or videos from online databases (e.g.
      YouTube). We will record EEG signals and attempt to interpret them in
      accordance to the VR scenarios being watched.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_eyetracking.png" />
        <h2>Now You See It, Now You Don't: Virtual Reality and Vision</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Nico Marek</li>
        <li class="icon-contact">nico.marek@ovgu.de</li>
        <li class="icon-link"><a href="https://pupil-labs.com/vr-ar/">Pupil Labs</a></li>
        <li class="icon-link"><a href="https://unity3d.com/unity">Unity</a></li>
      </ul>
      <p>Virtual reality enables us not only to create artificial environments,
      we can also manipulate what &mdash; and how &mdash; participants are
      seeing. For this <span class="project">project</span>, we will try to
      implement artificial limitations to vision (with and without
      VR-eye-tracking), create basic environments to explore and interact with,
      and implement basic experimental procedures to collect behavioral data in
      Unity3D.</p>
    </section>

    <section class="card tri pitch">
      <header>
        <img src="/theme/img/pitch_bug.png" />
        <h2>Find The Bug: The Other Human in The Loop</h2>
      </header>
      <ul class="info">
        <li class="icon-people">Daniel Kottke</li>
        <li class="icon-contact">daniel.kottke@uni-kassel.de</li>
        <li class="icon-link"><a href="https://www.ies.uni-kassel.de/p/bug-study/">Website</a></li>
      </ul>
      <p>Recently, we conducted a pilot study with humans to find selection
      strategies for a machine learning task (active learning). In this project,
      we develop the concept of an advanced version of the previously used paper
      card game extending the existing study. Furthermore, we take a look into
      existing data from the previous study to state interesting research
      hypotheses.</p>
    </section>



    <footer class='card'>
      <p>content licensed <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">cc-by-sa</a>
        <span class='break'>—</span> &copy;2018 BrainHack Magdeburg <span class='break'>—</span>
        unless <a rel="license" href='./copyright.html'>indicated otherwise</a>
      </p>
    </footer>
  </main>
</body>
</html>