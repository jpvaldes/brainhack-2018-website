<html>
<head>
  <title>Project Ideas</title>
  <meta name="save_as" content="projects.html" />
  <meta name="url" content="projects.html" />
</head>
<body>
  <header id='project-landing' class='card'>
    <div class="wrapper">
      <div class='blurb'>
        <h1>Project <strong>Ideas</strong></h1>
        <p>For now these are only ideas. You can help make them happen.</p>
      </div>
    </div>
  </header>
  <div class='projects'>
<!-- Available info icons:
     <li class='icon-project'><a href='#'>Project Page</a></li>
     <li class='icon-people'>Name, Another Name, So Popular</li>
     <li class='icon-contact'>email@address.com</li>
     <li class='icon-link'><a href='#'>a link</a></li>
-->
    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_datalad.png' />
        <h2>Research Data Management with DataLad</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Michael Hanke</li>
        <li class='icon-contact'>michael.hanke@gmail.com</li>
        <li class='icon-link'><a href='http://datalad.org'>datalad.org</a></li>
      </ul>
      <p>Research data management is crucial for projects of any size. This
      <span class='tutorial'>tutorial</span> introduces you to DataLad, an
      open-source solution for decentralized data management that is build atop
      Git. DataLad's features make it a perfect fit for tracking your research
      output, whether it's produced on your laptop or in the "cloud". DataLad
      helps you get ready for open-science from the very start.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_pymvpa.png' />
        <h2>BIDS-app-Based Searchlight Analysis Pipeline</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Michael Hanke</li>
        <li class='icon-contact'>michael.hanke@gmail.com</li>
        <li class='icon-link'><a href='http://bids-apps.neuroimaging.io'>BIDS-app</a></li>
        <li class='icon-link'><a href='http://www.pymvpa.org'>PyMVPA</a></li>
      </ul>
      <p>Decoding analysis using a traveling "searchlight" are a widely used
      approach to map the availability of a given signal, and can also be used
      for a representational similarity analysis (RSA). This
      <span class='project'>project</span> aims to produce a BIDS-app for a
      PyMVPA-based searchlight analysis for decoding and RSA.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_ph_brain.png' />
        <h2>Basics of Image Reconstruction: From Raw Data to the Image</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Falk Lüsebrink</li>
        <li class='icon-contact'>falk.luesebrink@ovgu.de</li>
      </ul>
      <p>In this <span class='project'>project</span>, a basic reconstruction
      pipeline is to be developed for Cartesian sampled gradient echo data. This
      includes reading raw data, k space filtering, data synthesis in case of
      partial Fourier, coil combination (sum of squares and adaptive
      combination) and writing of image files. The aim is to reconstruct image
      with at least the quality of the vendors.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_nipype.png' />
        <h2>Containerized Nipype Pipelines on the Cluster</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>José P. Valdés-Herrera</li>
        <li class='icon-contact'>jose.valdes@dzne.de</li>
        <li class='icon-link'><a href='http://nipype.readthedocs.io/'>Nipype</a></li>
        <li class='icon-link'><a href='https://www.docker.com'>Docker</a></li>
        <li class='icon-link'><a href='http://singularity.lbl.gov'>Singularity</a></li>
      </ul>
      <p>This <span class='project'>project</span> proposes taking custom or
      already available pipelines developed using Nipype, package them in a
      container using Docker or Singularity, and integrate them with cluster
      software (SLURM, SGE, Condor). The aim is to automate the process of
      taking those pipelines to a cluster as much as possible, either by
      developing scripts or extending Nipype's functionality.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_dask.png' />
        <h2>Machine Learning with Dask and Neuroimaging Data</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>José P. Valdés-Herrera</li>
        <li class='icon-contact'>jose.valdes@dzne.de</li>
        <li class='icon-link'><a href='https://dask.pydata.org/'>Dask</a></li>
      </ul>
      <p>The aim of this <span class='project'>project</span> is to analyse
      large neuroimaging datasets &mdash; that do not fit in memory &mdash; even
      without access to powerful machines or clusters. To achieve this, we can
      use machine learning libraries (Nilearn, PyMVPA, scikit-learn) together
      with a library like Dask. During the event, the goal is to integrate
      these tools, pick a large dataset, and analyse it on "modest" hardware
      (e.g. a laptop).</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_ph_limes.png' />
        <h2>Motor Imagery Classification with Deep Neural Networks</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>JiaHua Xu</li>
        <li class='icon-contact'>jiahua.xu@med.ovgu.de</li>
        <li class='icon-link'><a href='http://www.bbci.de/competition/iv/#dataset'>Datasets</a></li>
        <li class='icon-link'><a href='https://www.martinos.org/mne/stable/index.html'>MNE</a></li>
        <li class='icon-link'><a href='http://scikit-learn.org/stable/'>scikit-learn</a></li>
      </ul>
      <p>Motor imagery represents frequency phenomenon of Event-Related
      Desynchronization (ERD) and Event-Related Synchronization (ERS) in the
      motor cortex. In this <span class='tutorial'>tutorial</span>, you will
      jointly train a logistic regression and a deep neural network combined
      model (Deep&amp;Wide Learning) to evaluate the performance on four classes
      of motor imagery data.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_ph_paint.png' />
        <h2>Brain Network Analysis Pipelines in Neuroimaging Data</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>JiaHua Xu</li>
        <li class='icon-contact'>jiahua.xu@med.ovgu.de</li>
        <li class='icon-link'><a href='http://www.fieldtriptoolbox.org/'>FieldTrip</a></li>
        <li class='icon-link'><a href='https://www.martinos.org/mne/stable/index.html'>MNE</a></li>
        <li class='icon-link'><a href='https://sites.google.com/site/bctnet/'>Brain Connectivity Toolbox</a></li>
      </ul>
      <p>This <span class='project'>project</span> will focus on the pipelines
      of brain network analysis when using EEG data with different kinds of
      graphics measures, from mathematics to practical. The aim is to help
      visualize the brain dynamic connectivity changes and understand the
      network structures of brain activities in a different perspective.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_brain.png' />
        <h2>Highest Resolution T1-weighted In Vivo Human Brain MRI Data</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Falk Lüsebrink</li>
        <li class='icon-contact'>falk.luesebrink@ovgu.de</li>
        <li class='icon-link'><a href='http://www.nature.com/articles/sdata201732'>publication</a></li>
      </ul>
      <p>In this <span class='tutorial'>tutorial</span>, the so far highest
      resolution T1-weighted in vivo human brain MRI data will be presented.
      Details will be given on what the entire data set consists of, how it was
      acquired, how it will be extended in the near future, and what potential
      use cases of it are. Besides simply showing some astonishing images of the
      brain.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_ph_dots.png' />
        <h2>How to Open Data (at OvGU)</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Falk Lüsebrink</li>
        <li class='icon-contact'>falk.luesebrink@ovgu.de</li>
      </ul>
      <p>Sharing your data openly is fantastic and may be beneficial for
      everyone. However, to improve visibility and enable citeability, data has
      to be provided with a digital object identifier (DOI). This <span
      class='tutorial'>tutorial</span> will guide you through the steps to
      successfully apply for a DOI at the OvGU university library. Starting from
      the application itself, how and where to host data, and the regulations.
      Non-OvGU-members will learn how your institute is able to provide
      DOIs.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_audio.png' />
        <h2>Hear What I Hear: Reconstructing the Music in Our Heads</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Andrew Curran</li>
        <li class='icon-contact'>andrew.curran@med.ovgu.de</li>
        <li class='icon-link'><a href='http://www.eneuro.org/content/early/2018/01/29/ENEURO.0358-17.2018'>Visual Image Reconstruction</a></li>
      </ul>
      <p>Recently, research groups have shown that it is possible to read images
      from a person's brain as they are seen using both fMRI and EEG through
      machine learning strategies. These strategies have however not yet been
      applied to auditory stimuli. It should theoretically also be possible to
      record imagined visual or auditory perceptions. This
      <span class='project'>project</span> aims to utilize the same principles
      to extract simple musical details through EEG, and potentially even
      imagined music.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_nipype2.png' />
        <h2>Nipype Tutorial</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Michael Notter</li>
        <li class='icon-contact'>michaelnotter@hotmail.com</li>
        <li class='icon-link'><a href='https://miykael.github.io/nipype_tutorial/'>Nipype Tutorial</a></li>
      </ul>
      <p>This <span class='tutorial'>tutorial</span> introduces you to Nipype,
      an open-source neuroimaging software written in Python. Nipype provides a
      unified way of interfacing with heterogeneous neuroimaging softwares like
      SPM, FSL, FreeSurfer, AFNI, ANTS, Camino, and many more. It allows users
      to create flexible and complex workflows very quickly, that then can be
      executed in parallel either locally or on any computational cluster.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_3d.png' />
        <h2>3D-Printing for Science</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Michael Lippert</li>
        <li class='icon-contact'>mlippert@lin-magdeburg.de</li>
      </ul>
      <p>Additive manufacturing provides exciting opportunities in the lab:
         need a protoype? Just print it! More of these super expensive small
         parts that evil ACME Corp overcharges you for? 3D-printers for the
         rescue! In this <span class='project'>project</span> we will take a
         look at the workflow of FDM 3D printing and how you can implement it in
         your lab.</p>
    </section>

    <section class='pitch'>
      <header>
        <img src='/theme/img/pitch_vr.png' />
        <h2>Seeing is Believing: Connecting VR Environments to EEG Output Interpretation</h2>
      </header>
      <ul class='info'>
        <li class='icon-people'>Siew-Wan Ohl; JiaHua Xu</li>
        <li class='icon-contact'>siew-wan.ohl@ovgu.de; jiahua.xu@med.ovgu.de</li>
        <li class='icon-link'><a href='http://www.edu2vr.com'>Edu2VR</a></li>
        <li class='icon-link'><a href='http://openbci.com'>OpenBCI</a></li>
      </ul>
      <p>For this <span class='project'>project</span>, we would like to see
      what EEG signals will be generated when different VR environments or
      scenarios are presented to the user. It is up to the creativity of the
      programmer to either generate interesting VR environments or directly
      import existing 360 degree photos or videos from online databases (e.g.
      YouTube). We will record EEG signals and attempt to interpret them in
      accordance to the VR scenarios being watched.</p>
    </section>
  </div>
</body>
</html>
